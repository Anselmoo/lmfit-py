{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ndoc_fitting_emcee.py\n====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "##\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n##\n# <examples/doc_fitting_emcee.py>\nimport numpy as np\n\nimport lmfit\n\ntry:\n    import matplotlib.pyplot as plt\n    HASPYLAB = True\nexcept ImportError:\n    HASPYLAB = False\n\ntry:\n    import corner\n    HASCORNER = True\nexcept ImportError:\n    HASCORNER = False\n\nx = np.linspace(1, 10, 250)\nnp.random.seed(0)\ny = (3.0*np.exp(-x/2) - 5.0*np.exp(-(x-0.1) / 10.) +\n     0.1*np.random.randn(x.size))\nif HASPYLAB:\n    plt.plot(x, y, 'b')\n    plt.show()\n\np = lmfit.Parameters()\np.add_many(('a1', 4), ('a2', 4), ('t1', 3), ('t2', 3., True))\n\n\ndef residual(p):\n    v = p.valuesdict()\n    return v['a1']*np.exp(-x/v['t1']) + v['a2']*np.exp(-(x-0.1) / v['t2']) - y\n\n\nmi = lmfit.minimize(residual, p, method='nelder', nan_policy='omit')\nlmfit.printfuncs.report_fit(mi.params, min_correl=0.5)\nif HASPYLAB:\n    plt.figure()\n    plt.plot(x, y, 'b')\n    plt.plot(x, residual(mi.params) + y, 'r', label='best fit')\n    plt.legend(loc='best')\n    plt.show()\n\n# Place bounds on the ln(sigma) parameter that emcee will automatically add\n# to estimate the true uncertainty in the data since is_weighted=False\nmi.params.add('__lnsigma', value=np.log(0.1), min=np.log(0.001), max=np.log(2))\n\nres = lmfit.minimize(residual, method='emcee', nan_policy='omit', burn=300,\n                     steps=1000, thin=20, params=mi.params, is_weighted=False)\n\nif HASPYLAB and HASCORNER:\n    emcee_corner = corner.corner(res.flatchain, labels=res.var_names,\n                                 truths=list(res.params.valuesdict().values()))\n    plt.show()\n\nprint(\"\\nmedian of posterior probability distribution\")\nprint('--------------------------------------------')\nlmfit.report_fit(res.params)\n\n# find the maximum likelihood solution\nhighest_prob = np.argmax(res.lnprob)\nhp_loc = np.unravel_index(highest_prob, res.lnprob.shape)\nmle_soln = res.chain[hp_loc]\nfor i, par in enumerate(p):\n    p[par].value = mle_soln[i]\nprint(\"\\nMaximum likelihood Estimation\")\nprint('-----------------------------')\nfor _, vals in p.items():\n    print(vals)\n\nif HASPYLAB:\n    plt.figure()\n    plt.plot(x, y, 'b')\n    plt.plot(x, residual(mi.params) + y, 'r', label='Nelder-Mead')\n    plt.plot(x, residual(res.params) + y, 'k--', label='emcee')\n    plt.legend()\n    plt.show()\n\nquantiles = np.percentile(res.flatchain['t1'], [2.28, 15.9, 50, 84.2, 97.7])\nprint(\"1 sigma spread\", 0.5 * (quantiles[3] - quantiles[1]))\nprint(\"2 sigma spread\", 0.5 * (quantiles[4] - quantiles[0]))\n# <end of examples/doc_fitting_emcee.py>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}